{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, datasets, backend\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset API 설정(train에 적용시키는 코드 못찾음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 생성 : tf.data.Dataset을 생성하는 것으로 메모리에 한번에 로드하여 사용할 수도 있으며, 동적으로 전달하여 사용할 수도 있습니다.\n",
    "#Iterator 생성 : 데이터를 조회할때 사용되는 iterator 를 생성합니다.\n",
    "#데이터 사용 : 실제 모델에 데이터를 입력하거나, 읽게 됩니다.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#dataset api\n",
    "train, test = tf.keras.datasets.cifar10.load_data() #데이터불러오기\n",
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))#데이터셋 생성코드\n",
    "#dataset = dataset.shuffle(100000).repeat().batch(10) \n",
    "#위의 코드에서 생성된 Dataset 을 shuffle 함수를 이용하여 섞습니다. shuffle 함수는 고정된 버퍼 크기로 데이터를 섞는데, \n",
    "#데이터가 완전히 랜덤적으로 뒤섞기 위해서는 입력된 데이터 크기보다 큰 수를 입력해 주셔야 합니다.\n",
    "#repeat라는 함수는 데이터셋을 읽다가 마지막에 도달했을 경우, 다시 처음부터 조회하는 함수입니다. \n",
    "#batch 함수는 데이터를 읽어올 개수를 지정하는 함수입니다.\n",
    "iterator = dataset.make_one_shot_iterator()#one-shot iterator 생성,iterator 상태를 처음 초기화하거나 다시 초기화 하는 동작을 합니다.\n",
    "#next_element = iterator.get_next() #다음 항목에 연결되어 있는 tf.Tensor 객체를 리턴합니다\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "    #print (sess.run(next_element)) #세션을 실행시켜서, 데이터를 가져와서 사용 할 수 있습니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functional api\n",
    "\n",
    "input = Input(shape=(32,32,3))\n",
    "conv1 = Conv2D(64,kernel_size=3,activation='relu',padding='same')(input)\n",
    "conv2 = Conv2D(64,kernel_size=3,activation='relu',padding='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "conv3 = Conv2D(128,kernel_size=3,activation='relu',padding='same')(pool1)\n",
    "conv4 = Conv2D(128,kernel_size=3,activation='relu',padding='same')(conv3)\n",
    "pool2 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "conv5 = Conv2D(256,kernel_size=3,activation='relu',padding='same')(pool2)\n",
    "conv6 = Conv2D(256,kernel_size=3,activation='relu',padding='same')(conv5)\n",
    "conv7 = Conv2D(256,kernel_size=3,activation='relu',padding='same')(conv6)\n",
    "pool3 = MaxPooling2D(pool_size=(2,2))(conv7)\n",
    "conv8 = Conv2D(512,kernel_size=3,activation='relu',padding='same')(pool3)\n",
    "conv9 = Conv2D(512,kernel_size=3,activation='relu',padding='same')(conv8)\n",
    "conv10 = Conv2D(512,kernel_size=3,activation='relu',padding='same')(conv9)\n",
    "pool4= MaxPooling2D(pool_size=(2,2))(conv10)\n",
    "conv11 = Conv2D(512,kernel_size=3,activation='relu',padding='same')(pool4)\n",
    "conv12= Conv2D(512,kernel_size=3,activation='relu',padding='same')(conv11)\n",
    "conv13 = Conv2D(512,kernel_size=3,activation='relu',padding='same')(conv12)\n",
    "pool5= MaxPooling2D(pool_size=(2,2))(conv13)\n",
    "flat = Flatten()(pool5)\n",
    "dense1 = Dense(4096,activation='relu')(flat)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(4096,activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "output = Dense(10,activation='softmax')(drop2)\n",
    "model = Model(inputs = input,outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics=['acc']) # For a multi-class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.shuffle(100000).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(dataset, epochs=10, steps_per_epoch=30)\n",
    "# model.fit(iterator, epochs=10, steps_per_epoch=30)\n",
    "model.fit(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = d.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(iterator, steps_per_epoch=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 끝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\n",
    "#기능별 정규화에 필요한 계산량\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# 실시간 데이터 기능 보강을 사용하여 배치에 모델을 fit :\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(x_train) / 32, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset API, augmentation 없이 훈련 시키는 내용(아래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, epochs = 1, batch_size =64) \n",
    "#이 코드는 작동함 - 지난주에 했던 코드라서 끝까지 안돌림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터증식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터증식 제너레이터를 사용해서 훈련시키기\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일경로 지정하기\n",
    "#파일 내 test, train 이 제대로 안나눠져있고, \n",
    "#train_data는 data_batch_1~5 이런식으로 되어있음.그리고 저걸 넣으면 아래 코드가 오류남 ->format이 달라서 처리가 안됨 \n",
    "train_dir = r'C:/Users/Affinity/cifar-10-batches-py' \n",
    "test_dir = r'C:/Users/Affinity/cifar-10-batches-py/test_batch'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen.flow_from_directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#여기서부터 문제 안풀림\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(150,150),batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#여기도 어떻게 해결해야할지 모르겠음.\n",
    "history = model.fit(train_generator, steps_per_epoch=50, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(x_train) / 32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(x_train, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
